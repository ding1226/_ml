from sklearn.ensemble import IsolationForest
from sklearn.datasets import make_blobs
import numpy as np

# 生成正常資料（兩群聚類）
X, _ = make_blobs(n_samples=300, centers=1, cluster_std=0.5, random_state=42)

# 補充少量異常點
outliers = np.random.uniform(low=-6, high=6, size=(20, 2))
X_with_outliers = np.vstack([X, outliers])

# 建立 Isolation Forest 模型
model = IsolationForest(contamination=0.06, random_state=42)
model.fit(X_with_outliers)

# 預測：正常為1，異常為-1
preds = model.predict(X_with_outliers)

# 印出異常點索引
print("偵測出的異常點索引:", np.where(preds == -1)[0])

Isolation Forest 原理簡介
孤立森林基於一個簡單直覺：異常點「比較容易被孤立」。

方法用隨機切割資料的方式建多棵樹（Isolation Trees），

在樹結構中，異常點會因為與其他資料差異大而較快被切割孤立，路徑較短。

利用多棵樹的平均「孤立路徑長度」來判斷資料點是否異常。路徑越短，越可能是異常。

-----
  
為什麼用 Isolation Forest？
不需監督標籤，適合異常點很少、難標註的情況。

演算法效率高，適合大資料。

比較不用擔心異常點分布形狀，不像密度方法受限。
